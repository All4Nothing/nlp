# Natural Language Processing Repo
## Paper

|        Title         |        Year         |        Keywords         |        Link         |
| :-----: | :-----: | :-----: | :-----: |
|  NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE  | 2014 | #Attetnion |<a href="https://github.com/All4Nothing/papers-repo/tree/main/NEURAL%20MACHINE%20TRANSLATION%20BY%20JOINTLY%20LEARNING%20TO%20ALIGN%20AND%20TRANSLATE">Link</a> |
|  Effective Approaches to Attention-based Neural Machine Translation  | 2015 | #Attention |<a href="https://github.com/All4Nothing/papers-repo/tree/main/Effective%20Approaches%20to%20Attention-based%20Neural%20Machine%20Translation">Link</a> |
|  Attention Is All You Need  | 2017 | #Attetnion #Transformer |<a href="https://github.com/All4Nothing/papers-repo/tree/main/Attention%20Is%20All%20You%20Need">Link</a> |
|  Improving Language Understanding by Generative Pre-Training  | 2018 | #GPT-1 #LLM |<a href="https://github.com/All4Nothing/papers-repo/tree/main/Improving%20Language%20Understanding%20by%20Generative%20Pre-Training">Link</a> |
|  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  | 2018 | #BERT #LLM |<a href="https://github.com/All4Nothing/papers-repo/tree/main/BERT%3A%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding">Link</a> |
|  Language Models are Unsupervised Multitask Learners  | 2020 | #GPT-2 #LLM |<a href="https://github.com/All4Nothing/papers-repo/tree/main/Language%20Models%20are%20Unsupervised%20Multitask%20Learners">Link</a> |
|  Language Models are Few-Shot Learners  | 2020 | #GPT-3 #LLM |<a href="https://github.com/All4Nothing/papers-repo/tree/main/Language%20Models%20are%20Few-Shot%20Learners">Link</a> |
